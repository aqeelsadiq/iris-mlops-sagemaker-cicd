# name: SageMaker Model Monitor (Iris) - Production

# on:
#   workflow_dispatch:

# jobs:
#   model_monitor:
#     runs-on: ubuntu-latest

#     env:
#       AWS_REGION: us-east-1

#       # Your endpoint + role
#       ENDPOINT_NAME: iris-endpoint
#       SAGEMAKER_EXEC_ROLE_ARN: arn:aws:iam::387867038403:role/AqeelIrisSageMakerExecutionRole

#       # Buckets / paths
#       S3_BUCKET: sagemaker-aqeel-iris-us-east-1-387867038403
#       DATACAPTURE_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/datacapture/

#       # Baseline dataset (features only, header=True)
#       # BASELINE_DATASET_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/datasets/iris/features.csv
#       BASELINE_DATASET_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/datasets/iris/baseline/features.csv
#       BASELINE_OUTPUT_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/baseline/

#       # Model Monitor reports
#       REPORTS_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/reports

#       # Monitoring schedule
#       SCHEDULE_NAME: aqeel-iris-model-monitor
#       CRON: "cron(0 * ? * * *)"
#       PROCESSING_INSTANCE_TYPE: ml.t3.large

#       # Alerting (SNS topic name)
#       SNS_TOPIC_NAME: aqeel-iris-drift-alerts

#     steps:
#       - name: Checkout
#         uses: actions/checkout@v4

#       - name: Configure AWS Credentials
#         uses: aws-actions/configure-aws-credentials@v4
#         with:
#           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
#           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
#           aws-region: ${{ env.AWS_REGION }}

#       - name: Setup Python
#         uses: actions/setup-python@v5
#         with:
#           python-version: "3.11"

#       - name: Install dependencies
#         run: |
#           python -m pip install --upgrade pip
#           pip install -r monitoring/requirements.txt

#       # 1) Enable DataCapture on the endpoint (idempotent)
#       - name: Enable Data Capture (EndpointConfig update)
#         run: |
#           python monitoring/enable_data_capture.py \
#             --region "${{ env.AWS_REGION }}" \
#             --endpoint-name "${{ env.ENDPOINT_NAME }}" \
#             --capture-s3-uri "${{ env.DATACAPTURE_S3_URI }}" \
#             --sampling-percentage 100

#       # 2) Create / refresh baseline (statistics.json + constraints.json)
#       - name: Create/Update Baseline (Model Monitor)
#         run: |
#           python monitoring/mm_create_baseline.py \
#             --region "${{ env.AWS_REGION }}" \
#             --role-arn "${{ env.SAGEMAKER_EXEC_ROLE_ARN }}" \
#             --baseline-data-s3-uri "${{ env.BASELINE_DATASET_S3_URI }}" \
#             --baseline-output-s3-uri "${{ env.BASELINE_OUTPUT_S3_URI }}" \
#             --instance-type "${{ env.PROCESSING_INSTANCE_TYPE }}"

#       # 3) Create / recreate monitoring schedule
#       - name: Create/Update Monitoring Schedule (Model Monitor)
#         run: |
#           python monitoring/mm_create_schedule.py \
#             --region "${{ env.AWS_REGION }}" \
#             --role-arn "${{ env.SAGEMAKER_EXEC_ROLE_ARN }}" \
#             --endpoint-name "${{ env.ENDPOINT_NAME }}" \
#             --schedule-name "${{ env.SCHEDULE_NAME }}" \
#             --baseline-s3-uri "${{ env.BASELINE_OUTPUT_S3_URI }}" \
#             --monitor-output-s3-uri "${{ env.REPORTS_S3_URI }}" \
#             --datacapture-s3-uri "${{ env.DATACAPTURE_S3_URI }}" \
#             --cron "${{ env.CRON }}" \
#             --instance-type "${{ env.PROCESSING_INSTANCE_TYPE }}" \
#             --instance-count 1 \
#             --volume-size 50 \
#             --max-runtime 1800

#       # 4) Create SNS + CloudWatch alarm for drift (constraint violations)
#       # IMPORTANT: add GitHub secret ALERT_EMAIL (your email) and confirm subscription from inbox.
#       - name: Create Drift Alarm (CloudWatch + SNS)
#         run: |
#           python monitoring/mm_create_drift_alarm.py \
#             --region "${{ env.AWS_REGION }}" \
#             --schedule-name "${{ env.SCHEDULE_NAME }}" \
#             --endpoint-name "${{ env.ENDPOINT_NAME }}" \
#             --sns-topic-name "${{ env.SNS_TOPIC_NAME }}" \
#             --email "aqeel.sadiq3456@gmail.com"



#       # - name: Create Drift Alarm (CloudWatch + SNS)
#       #   run: |
#       #     python monitoring/mm_create_drift_alarm.py \
#       #       --region "${{ env.AWS_REGION " \
#       #       --schedule-name "${{ env.SCHEDULE_NAME " \
#       #       --sns-topic-name "${{ env.SNS_TOPIC_NAME " \
#       #       --email "aqeel.sadiq3456@gmail.com"





#claude code
name: SageMaker Model Monitor (Iris) - Production

on:
  workflow_dispatch:

jobs:
  model_monitor:
    runs-on: ubuntu-latest

    env:
      AWS_REGION: us-east-1

      # Endpoint + IAM role
      ENDPOINT_NAME: iris-endpoint
      SAGEMAKER_EXEC_ROLE_ARN: arn:aws:iam::387867038403:role/AqeelIrisSageMakerExecutionRole

      # S3 bucket and paths
      S3_BUCKET: sagemaker-aqeel-iris-us-east-1-387867038403
      DATACAPTURE_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/datacapture/
      BASELINE_DATASET_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/datasets/iris/baseline/features.csv
      BASELINE_OUTPUT_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/baseline/
      REPORTS_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/reports/

      # ✅ Preprocessor script — uploaded to S3 by mm_create_baseline.py
      #    MUST be the same URI passed to both baseline AND schedule steps
      PREPROCESSOR_LOCAL_PATH: ./monitoring/record_preprocessor.py
      PREPROCESSOR_S3_URI: s3://sagemaker-aqeel-iris-us-east-1-387867038403/monitoring/scripts/record_preprocessor.py

      # Monitoring schedule
      SCHEDULE_NAME: aqeel-iris-model-monitor
      CRON: "cron(0 * ? * * *)"
      PROCESSING_INSTANCE_TYPE: ml.c5.large

      # Drift alerting
      SNS_TOPIC_NAME: aqeel-iris-drift-alerts
      ALERT_EMAIL: aqeel.sadiq3456@gmail.com

    steps:
      # ── Setup ────────────────────────────────────────────────────────────────

      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r monitoring/requirements.txt

      # ── Step 1: Enable Data Capture ──────────────────────────────────────────
      # Idempotent — creates a new EndpointConfig with DataCapture enabled
      # and updates the endpoint to use it. Captures both Input + Output.

      - name: Enable Data Capture on Endpoint
        run: |
          python monitoring/enable_data_capture.py \
            --region           "${{ env.AWS_REGION }}" \
            --endpoint-name    "${{ env.ENDPOINT_NAME }}" \
            --capture-s3-uri   "${{ env.DATACAPTURE_S3_URI }}" \
            --sampling-percentage 100

      # ── Step 2: Create Baseline ───────────────────────────────────────────────
      # Uploads record_preprocessor.py to S3, then runs suggest_baseline()
      # using the same preprocessor so baseline column schema matches live data.
      # Outputs: statistics.json + constraints.json → BASELINE_OUTPUT_S3_URI

      - name: Create Baseline (statistics + constraints)
        run: |
          python monitoring/mm_create_baseline.py \
            --region                  "${{ env.AWS_REGION }}" \
            --role-arn                "${{ env.SAGEMAKER_EXEC_ROLE_ARN }}" \
            --baseline-data-s3-uri    "${{ env.BASELINE_DATASET_S3_URI }}" \
            --baseline-output-s3-uri  "${{ env.BASELINE_OUTPUT_S3_URI }}" \
            --preprocessor-local-path "${{ env.PREPROCESSOR_LOCAL_PATH }}" \
            --preprocessor-s3-uri     "s3://${{ env.S3_BUCKET }}/monitoring/scripts/" \
            --instance-type           "${{ env.PROCESSING_INSTANCE_TYPE }}" \
            --instance-count          1

      # ── Step 3: Create Monitoring Schedule ───────────────────────────────────
      # Deletes any existing schedule and recreates it.
      # Uses the SAME preprocessor S3 URI as the baseline step — this is critical.

      - name: Create Monitoring Schedule
        run: |
          python monitoring/mm_create_schedule.py \
            --region                 "${{ env.AWS_REGION }}" \
            --role-arn               "${{ env.SAGEMAKER_EXEC_ROLE_ARN }}" \
            --endpoint-name          "${{ env.ENDPOINT_NAME }}" \
            --schedule-name          "${{ env.SCHEDULE_NAME }}" \
            --baseline-s3-uri        "${{ env.BASELINE_OUTPUT_S3_URI }}" \
            --monitor-output-s3-uri  "${{ env.REPORTS_S3_URI }}" \
            --datacapture-s3-uri     "${{ env.DATACAPTURE_S3_URI }}" \
            --preprocessor-s3-uri    "${{ env.PREPROCESSOR_S3_URI }}" \
            --cron                   "${{ env.CRON }}" \
            --instance-type          "${{ env.PROCESSING_INSTANCE_TYPE }}" \
            --instance-count         1 \
            --volume-size            50 \
            --max-runtime            1800

      # ── Step 4: Create Drift Alarms ───────────────────────────────────────────
      # Creates CloudWatch alarms (one per feature) + SNS email subscription.
      #  Confirm the subscription email that arrives in your inbox (once only).

      - name: Create Drift Alarms (CloudWatch + SNS)
        run: |
          python monitoring/mm_create_drift_alarm.py \
            --region          "${{ env.AWS_REGION }}" \
            --schedule-name   "${{ env.SCHEDULE_NAME }}" \
            --endpoint-name   "${{ env.ENDPOINT_NAME }}" \
            --sns-topic-name  "${{ env.SNS_TOPIC_NAME }}" \
            --email           "${{ env.ALERT_EMAIL }}" \
            --features        "sepal_length,sepal_width,petal_length,petal_width" \
            --period          3600 \
            --eval-periods    1 \
            --threshold       1.0